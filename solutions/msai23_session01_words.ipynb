{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaletov/math-basics-for-ai/blob/master/solutions/msai23_session01_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAmUD1eZHryf"
      },
      "source": [
        "# **Word vectors**\n",
        "\n",
        "\n",
        "In the previous exercise we observed that colors that we think of as similar are 'closer' to each other in RGB vector space. Is it possible to create a vector space for all English words that has this same 'closer in space is closer in meaning' property?\n",
        "\n",
        "The answer is yes! Luckily, you don't need to create those vectors from scratch. Many researchers have made downloadable databases of pre-trained vectors. One such project is [Stanford's Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/).\n",
        "\n",
        "These $300$-dimensional vectors are included with $\\texttt{spaCy}$, and they're the vectors we'll be using in this exercise.\n",
        "\n",
        "![cosine similarity: picture](https://d33wubrfki0l68.cloudfront.net/d2742976a92aa4d6c39f19c747ec5f56ed1cec30/3803f/images/guide-to-word-vectors-with-gensim-and-keras_files/word2vec-king-queen-vectors.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymHr8XZIHsML",
        "outputId": "2d2a6345-7d81-4855-f322-446470ea82de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The following will download the language model.\n",
        "# Resart the runtime (Runtime -> Restart runtime) after running this cell\n",
        "# (and don't run it for the second time).\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7yqHuGJ6e5"
      },
      "source": [
        "Let's load the model now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-8rsSkSBU8C"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNf5FAkm3Ljj"
      },
      "source": [
        "## **Word vectors: the first glance**\n",
        "\n",
        "You can see the vector of any word in $\\texttt{spaCy}$' s vocabulary using the $\\texttt{vector}$ attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx-IzWxQAgNN",
        "outputId": "065022b7-087e-4186-944d-c4e9bc3a902d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# A 300-dimensional vector\n",
        "len(nlp('dog').vector)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8UP3QrxKZPG",
        "outputId": "b51d353c-89cc-4a8d-8e3c-8552cb8a2987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp('dog').vector"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.2330e+00,  4.2963e+00, -7.9738e+00, -1.0121e+01,  1.8207e+00,\n",
              "        1.4098e+00, -4.5180e+00, -5.2261e+00, -2.9157e-01,  9.5234e-01,\n",
              "        6.9880e+00,  5.0637e+00, -5.5726e-03,  3.3395e+00,  6.4596e+00,\n",
              "       -6.3742e+00,  3.9045e-02, -3.9855e+00,  1.2085e+00, -1.3186e+00,\n",
              "       -4.8886e+00,  3.7066e+00, -2.8281e+00, -3.5447e+00,  7.6888e-01,\n",
              "        1.5016e+00, -4.3632e+00,  8.6480e+00, -5.9286e+00, -1.3055e+00,\n",
              "        8.3870e-01,  9.0137e-01, -1.7843e+00, -1.0148e+00,  2.7300e+00,\n",
              "       -6.9039e+00,  8.0413e-01,  7.4880e+00,  6.1078e+00, -4.2130e+00,\n",
              "       -1.5384e-01, -5.4995e+00,  1.0896e+01,  3.9278e+00, -1.3601e-01,\n",
              "        7.7732e-02,  3.2218e+00, -5.8777e+00,  6.1359e-01, -2.4287e+00,\n",
              "        6.2820e+00,  1.3461e+01,  4.3236e+00,  2.4266e+00, -2.6512e+00,\n",
              "        1.1577e+00,  5.0848e+00, -1.7058e+00,  3.3824e+00,  3.2850e+00,\n",
              "        1.0969e+00, -8.3711e+00, -1.5554e+00,  2.0296e+00, -2.6796e+00,\n",
              "       -6.9195e+00, -2.3386e+00, -1.9916e+00, -3.0450e+00,  2.4890e+00,\n",
              "        7.3247e+00,  1.3364e+00,  2.3828e-01,  8.4388e-02,  3.1480e+00,\n",
              "       -1.1128e+00, -3.5598e+00, -1.2115e-01, -2.0357e+00, -3.2731e+00,\n",
              "       -7.7205e+00,  4.0948e+00, -2.0732e+00,  2.0833e+00, -2.2803e+00,\n",
              "       -4.9850e+00,  9.7667e+00,  6.1779e+00, -1.0352e+01, -2.2268e+00,\n",
              "        2.5765e+00, -5.7440e+00,  5.5564e+00, -5.2735e+00,  3.0004e+00,\n",
              "       -4.2512e+00, -1.5682e+00,  2.2698e+00,  1.0491e+00, -9.0486e+00,\n",
              "        4.2936e+00,  1.8709e+00,  5.1985e+00, -1.3153e+00,  6.5224e+00,\n",
              "        4.0113e-01, -1.2583e+01,  3.6534e+00, -2.0961e+00,  1.0022e+00,\n",
              "       -1.7873e+00, -4.2555e+00,  7.7471e+00,  1.0173e+00,  3.1626e+00,\n",
              "        2.3558e+00,  3.3589e-01, -4.4178e+00,  5.0584e+00, -2.4118e+00,\n",
              "       -2.7445e+00,  3.4170e+00, -1.1574e+01, -2.6568e+00, -3.6933e+00,\n",
              "       -2.0398e+00,  5.0976e+00,  6.5249e+00,  3.3573e+00,  9.5334e-01,\n",
              "       -9.4430e-01, -9.4395e+00,  2.7867e+00, -1.7549e+00,  1.7287e+00,\n",
              "        3.4942e+00, -1.6883e+00, -3.5771e+00, -1.9013e+00,  2.2239e+00,\n",
              "       -5.4335e+00, -6.5724e+00, -6.7228e-01, -1.9748e+00, -3.1080e+00,\n",
              "       -1.8570e+00,  9.9496e-01,  8.9135e-01, -4.4254e+00,  3.3125e-01,\n",
              "        5.8815e+00,  1.9384e+00,  5.7294e-01, -2.8830e+00,  3.8087e+00,\n",
              "       -1.3095e+00,  5.9208e+00,  3.3620e+00,  3.3571e+00, -3.8807e-01,\n",
              "        9.0022e-01, -5.5742e+00, -4.2939e+00,  1.4992e+00, -4.7080e+00,\n",
              "       -2.9402e+00, -1.2259e+00,  3.0980e-01,  1.8858e+00, -1.9867e+00,\n",
              "       -2.3554e-01, -5.4535e-01, -2.1387e-01,  2.4797e+00,  5.9710e+00,\n",
              "       -7.1249e+00,  1.6257e+00, -1.5241e+00,  7.5974e-01,  1.4312e+00,\n",
              "        2.3641e+00, -3.5566e+00,  9.2066e-01,  4.4934e-01, -1.3233e+00,\n",
              "        3.1733e+00, -4.7059e+00, -1.2090e+01, -3.9241e-01, -6.8457e-01,\n",
              "       -3.6789e+00,  6.6279e+00, -2.9937e+00, -3.8361e+00,  1.3868e+00,\n",
              "       -4.9002e+00, -2.4299e+00,  6.4312e+00,  2.5056e+00, -4.5080e+00,\n",
              "       -5.1278e+00, -1.5585e+00, -3.0226e+00, -8.6811e-01, -1.1538e+00,\n",
              "       -1.0022e+00, -9.1651e-01, -4.7810e-01, -1.6084e+00, -2.7307e+00,\n",
              "        3.7080e+00,  7.7423e-01, -1.1085e+00, -6.8755e-01, -8.2901e+00,\n",
              "        3.2405e+00, -1.6108e-01, -6.2837e-01, -5.5960e+00, -4.4865e+00,\n",
              "        4.0115e-01, -3.7063e+00, -2.1704e+00,  4.0789e+00, -1.7973e+00,\n",
              "        8.9538e+00,  8.9421e-01, -4.8128e+00,  4.5367e+00, -3.2579e-01,\n",
              "       -5.2344e+00, -3.9766e+00, -2.1979e+00,  3.5699e+00,  1.4982e+00,\n",
              "        6.0972e+00, -1.9704e+00,  4.6522e+00, -3.7734e-01,  3.9101e-02,\n",
              "        2.5361e+00, -1.8096e+00,  8.7035e+00, -8.6372e+00, -3.5257e+00,\n",
              "        3.1034e+00,  3.2635e+00,  4.5437e+00, -5.7290e+00, -2.9141e-01,\n",
              "       -2.0011e+00,  8.5328e+00, -4.5064e+00, -4.8276e+00, -1.1786e+01,\n",
              "        3.5607e-01, -5.7115e+00,  6.3122e+00, -3.6650e+00,  3.3597e-01,\n",
              "        2.5017e+00, -3.5025e+00, -3.7891e+00, -3.1343e+00, -1.4429e+00,\n",
              "       -6.9119e+00, -2.6114e+00, -5.9757e-01,  3.7847e-01,  6.3187e+00,\n",
              "        2.8965e+00, -2.5397e+00,  1.8022e+00,  3.5486e+00,  4.4721e+00,\n",
              "       -4.8481e+00, -3.6252e+00,  4.0969e+00, -2.0081e+00, -2.0122e-01,\n",
              "        2.5244e+00, -6.8817e-01,  6.7184e-01, -7.0466e+00,  1.6641e+00,\n",
              "       -2.2308e+00, -3.8960e+00,  6.1320e+00, -8.0335e+00, -1.7130e+00,\n",
              "        2.5688e+00, -5.2547e+00,  6.9845e+00,  2.7835e-01, -6.4554e+00,\n",
              "       -2.1327e+00, -5.6515e+00,  1.1174e+01, -8.0568e+00,  5.7985e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHWCqKcl55TY"
      },
      "source": [
        "## **Cosine similarity**\n",
        "\n",
        "**Cosine similarity** is a common way of assessing similarity between words in NLP. It is essentially defined as the cosine of the angle between the vectors representing the words of interest.\n",
        "\n",
        "Recall that the angle $\\phi$ between two non-zero vectors $u$ and $v$ can be computed as follows:\n",
        "\n",
        "$cos(\\phi) = \\frac{(u,v)}{||u||\\cdot||v||}$\n",
        "\n",
        "![](https://miro.medium.com/max/1394/1*_Bf9goaALQrS_0XkBozEiQ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoi6FvPgMWid"
      },
      "source": [
        "Define a function computing cosine similarity between two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpJS01dmvGbe"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine(v1, v2):\n",
        "  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEwtAXJRMc-s"
      },
      "source": [
        "Test your function by computing similarities of some random pairs of words, e.g. $dog$ and $puppy$ vs. $dog$ and $kitten$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RBooDbGvYOG",
        "outputId": "a840bd58-dde3-47d8-a9e5-643dfbdf0694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(cosine(nlp(\"dog\").vector, nlp(\"dog\").vector))\n",
        "print(cosine(nlp(\"dog\").vector, nlp(\"puppy\").vector))\n",
        "print(cosine(nlp(\"dog\").vector, nlp(\"kitten\").vector))\n",
        "print(cosine(nlp(\"dog\").vector, nlp(\"kindergarden\").vector))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0000001\n",
            "0.81076676\n",
            "0.6515032\n",
            "0.24433023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgHDwfx8Mu66"
      },
      "source": [
        "## **Loading the text**\n",
        "\n",
        "Let's load the full text of *Alice in Wonderland*. It will serve us as a corpus of English words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am8NoIl2zMXi"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Alice in Wonderland\n",
        "response = requests.get('https://www.gutenberg.org/files/11/11-0.txt')\n",
        "\n",
        "# If you prefer Dracula, load this instead:\n",
        "#response = requests.get('https://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
        "\n",
        "# Extracting separate words from the text\n",
        "doc = nlp(response.text)\n",
        "tokens = list(set([w.text for w in doc if w.is_alpha]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAwABf4nNNR3"
      },
      "source": [
        "Check out the content of $\\texttt{tokens}$ now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4B4FRR6NRzx",
        "outputId": "7426157f-9fed-4bdf-ae11-ea34211bee03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['atom',\n",
              " 'flame',\n",
              " 'consider',\n",
              " 'zigzag',\n",
              " 'feelings',\n",
              " 'reduced',\n",
              " 'applause',\n",
              " 'thousand',\n",
              " 'completely',\n",
              " 'In',\n",
              " 'quarrel',\n",
              " 'introduced',\n",
              " 'every',\n",
              " 'books',\n",
              " 'clearer',\n",
              " 'Little',\n",
              " 'but',\n",
              " 'rats',\n",
              " 'plan',\n",
              " 'occasional',\n",
              " 'Down',\n",
              " 'if',\n",
              " 'pairs',\n",
              " 'soldier',\n",
              " 'thoroughly',\n",
              " 'three',\n",
              " 'welcome',\n",
              " 'accidentally',\n",
              " 'Twinkle',\n",
              " 'walking',\n",
              " 'banks',\n",
              " 'fell',\n",
              " 'drink',\n",
              " 'muttered',\n",
              " 'consented',\n",
              " 'into',\n",
              " 'Knave',\n",
              " 'promise',\n",
              " 'rich',\n",
              " 'stop',\n",
              " 'finger',\n",
              " 'between',\n",
              " 'cheered',\n",
              " 'picking',\n",
              " 'rule',\n",
              " 'round',\n",
              " 'unwillingly',\n",
              " 'passed',\n",
              " 'flavour',\n",
              " 'ringlets',\n",
              " 'sorrow',\n",
              " 'secret',\n",
              " 'indignantly',\n",
              " 'growled',\n",
              " 'ferrets',\n",
              " 'nodded',\n",
              " 'empty',\n",
              " 'our',\n",
              " 'Then',\n",
              " 'puppy',\n",
              " 'time',\n",
              " 'taking',\n",
              " 'delightful',\n",
              " 'delay',\n",
              " 'Lastly',\n",
              " 'tones',\n",
              " 'experiment',\n",
              " 'beautiful',\n",
              " 'Bill',\n",
              " 'roses',\n",
              " 'wrote',\n",
              " 'out',\n",
              " 'crown',\n",
              " 'languid',\n",
              " 'clasped',\n",
              " 'form',\n",
              " 'hastily',\n",
              " 'natured',\n",
              " 'doubt',\n",
              " 'March',\n",
              " 'II',\n",
              " 'impatient',\n",
              " 'cares',\n",
              " 'or',\n",
              " 'filled',\n",
              " 'tumbled',\n",
              " 'leap',\n",
              " 'shutting',\n",
              " 'invited',\n",
              " 'sell',\n",
              " 'worm',\n",
              " 'pattering',\n",
              " 'morsel',\n",
              " 'holding',\n",
              " 'airs',\n",
              " 'livery',\n",
              " 'real',\n",
              " 'curious',\n",
              " 'brave',\n",
              " 'garden',\n",
              " 'shoes',\n",
              " 'agree',\n",
              " 'doubtfully',\n",
              " 'grow',\n",
              " 'throw',\n",
              " 'immense',\n",
              " 'righthand',\n",
              " 'taste',\n",
              " 'lifted',\n",
              " 'overhead',\n",
              " 'sneeze',\n",
              " 'thatched',\n",
              " 'never',\n",
              " 'PROJECT',\n",
              " 'running',\n",
              " 'Pigeon',\n",
              " 'Mock',\n",
              " 'court',\n",
              " 'machines',\n",
              " 'without',\n",
              " 'steam',\n",
              " 'undertone',\n",
              " 'produced',\n",
              " 'snorting',\n",
              " 'be',\n",
              " 'insolence',\n",
              " 'draggled',\n",
              " 'sixpence',\n",
              " 'smiled',\n",
              " 'housemaid',\n",
              " 'hearts',\n",
              " 'seated',\n",
              " 'King',\n",
              " 'burnt',\n",
              " 'depends',\n",
              " 'letters',\n",
              " 'Do',\n",
              " 'song',\n",
              " 'write',\n",
              " 'over',\n",
              " 'ran',\n",
              " 'deserved',\n",
              " 'suet',\n",
              " 'Illustration',\n",
              " 'said',\n",
              " 'warning',\n",
              " 'cut',\n",
              " 'narrow',\n",
              " 'sentenced',\n",
              " 'salt',\n",
              " 'station',\n",
              " 'tears',\n",
              " 'roughly',\n",
              " 'reaching',\n",
              " 'shaped',\n",
              " 'ordering',\n",
              " 'peeping',\n",
              " 'conversations',\n",
              " 'melancholy',\n",
              " 'rubbing',\n",
              " 'bent',\n",
              " 'caught',\n",
              " 'START',\n",
              " 'us',\n",
              " 'As',\n",
              " 'learned',\n",
              " 'accident',\n",
              " 'giving',\n",
              " 'cook',\n",
              " 'cup',\n",
              " 'Pig',\n",
              " 'directed',\n",
              " 'Nobody',\n",
              " 'rat',\n",
              " 'daisy',\n",
              " 'choking',\n",
              " 'put',\n",
              " 'lay',\n",
              " 'whispered',\n",
              " 'refused',\n",
              " 'breathe',\n",
              " 'became',\n",
              " 'live',\n",
              " 'authority',\n",
              " 'missed',\n",
              " 'prizes',\n",
              " 'hoarse',\n",
              " 'Yet',\n",
              " 'loving',\n",
              " 'look',\n",
              " 'guess',\n",
              " 'shape',\n",
              " 'longer',\n",
              " 'by',\n",
              " 'paw',\n",
              " 'loose',\n",
              " 'sharp',\n",
              " 'knot',\n",
              " 'Sounds',\n",
              " 'crouched',\n",
              " 'sneezing',\n",
              " 'Run',\n",
              " 'For',\n",
              " 'been',\n",
              " 'mistake',\n",
              " 'still',\n",
              " 'account',\n",
              " 'kill',\n",
              " 'violent',\n",
              " 'blew',\n",
              " 'ending',\n",
              " 'shiver',\n",
              " 'word',\n",
              " 'remembered',\n",
              " 'grumbled',\n",
              " 'annoy',\n",
              " 'Game',\n",
              " 'could',\n",
              " 'bone',\n",
              " 'undo',\n",
              " 'sat',\n",
              " 'turn',\n",
              " 'irritated',\n",
              " 'inkstand',\n",
              " 'folded',\n",
              " 'heads',\n",
              " 'skirt',\n",
              " 'uglify',\n",
              " 'sometimes',\n",
              " 'seven',\n",
              " 'saucepan',\n",
              " 'ornamented',\n",
              " 'loud',\n",
              " 'boots',\n",
              " 'mice',\n",
              " 'feebly',\n",
              " 'stupidest',\n",
              " 'golden',\n",
              " 'week',\n",
              " 'plainly',\n",
              " 'draw',\n",
              " 'miles',\n",
              " 'people',\n",
              " 'place',\n",
              " 'Before',\n",
              " 'comfits',\n",
              " 'thought',\n",
              " 'chorus',\n",
              " 'exclamation',\n",
              " 'folding',\n",
              " 'Poor',\n",
              " 'lady',\n",
              " 'trampled',\n",
              " 'thick',\n",
              " 'cold',\n",
              " 'toes',\n",
              " 'dropped',\n",
              " 'ago',\n",
              " 'bat',\n",
              " 'sweet',\n",
              " 'keep',\n",
              " 'brought',\n",
              " 'hanging',\n",
              " 'pressing',\n",
              " 'fidgeted',\n",
              " 'leaning',\n",
              " 'grey',\n",
              " 'cackled',\n",
              " 'wider',\n",
              " 'did',\n",
              " 'saying',\n",
              " 'expected',\n",
              " 'While',\n",
              " 'boxed',\n",
              " 'box',\n",
              " 'hint',\n",
              " 'whereupon',\n",
              " 'positively',\n",
              " 'Exactly',\n",
              " 'killing',\n",
              " 'dance',\n",
              " 'Suddenly',\n",
              " 'elbows',\n",
              " 'fighting',\n",
              " 'settle',\n",
              " 'occurred',\n",
              " 'already',\n",
              " 'wearily',\n",
              " 'growling',\n",
              " 'learnt',\n",
              " 'top',\n",
              " 'no',\n",
              " 'injure',\n",
              " 'succeeded',\n",
              " 'she',\n",
              " 'silence',\n",
              " 'enormous',\n",
              " 'against',\n",
              " 'hat',\n",
              " 'saucer',\n",
              " 'Party',\n",
              " 'pool',\n",
              " 'twelve',\n",
              " 'uncomfortable',\n",
              " 'neat',\n",
              " 'neighbouring',\n",
              " 'wrapping',\n",
              " 'precious',\n",
              " 'expression',\n",
              " 'Involved',\n",
              " 'dear',\n",
              " 'cake',\n",
              " 'absurd',\n",
              " 'excellent',\n",
              " 'spoon',\n",
              " 'tougher',\n",
              " 'stopped',\n",
              " 'alive',\n",
              " 'whiting',\n",
              " 'worth',\n",
              " 'started',\n",
              " 'straightening',\n",
              " 'making',\n",
              " 'almost',\n",
              " 'brushing',\n",
              " 'finishing',\n",
              " 'angry',\n",
              " 'difficulties',\n",
              " 'gloves',\n",
              " 'uncommonly',\n",
              " 'scroll',\n",
              " 'followed',\n",
              " 'own',\n",
              " 'marked',\n",
              " 'examining',\n",
              " 'soothing',\n",
              " 'ways',\n",
              " 'buttered',\n",
              " 'bats',\n",
              " 'Canterbury',\n",
              " 'remain',\n",
              " 'wine',\n",
              " 'Everything',\n",
              " 'rather',\n",
              " 'Alas',\n",
              " 'uncomfortably',\n",
              " 'mixed',\n",
              " 'winter',\n",
              " 'bleeds',\n",
              " 'pine',\n",
              " 'take',\n",
              " 'pebbles',\n",
              " 'listeners',\n",
              " 'rustling',\n",
              " 'ledge',\n",
              " 'expecting',\n",
              " 'bristling',\n",
              " 'angrily',\n",
              " 'slippery',\n",
              " 'weak',\n",
              " 'upset',\n",
              " 'complaining',\n",
              " 'shade',\n",
              " 'low',\n",
              " 'growls',\n",
              " 'manage',\n",
              " 'courage',\n",
              " 'tiny',\n",
              " 'invent',\n",
              " 'teapot',\n",
              " 'rising',\n",
              " 'course',\n",
              " 'capital',\n",
              " 'promised',\n",
              " 'charges',\n",
              " 'counting',\n",
              " 'circumstances',\n",
              " 'feeble',\n",
              " 'footman',\n",
              " 'eager',\n",
              " 'How',\n",
              " 'spirited',\n",
              " 'tale',\n",
              " 'tossing',\n",
              " 'leading',\n",
              " 'music',\n",
              " 'regular',\n",
              " 'it',\n",
              " 'company',\n",
              " 'vegetable',\n",
              " 'sign',\n",
              " 'wants',\n",
              " 'She',\n",
              " 'fair',\n",
              " 'agony',\n",
              " 'managing',\n",
              " 'few',\n",
              " 'mad',\n",
              " 'cakes',\n",
              " 'lap',\n",
              " 'RABBIT',\n",
              " 'figure',\n",
              " 'though',\n",
              " 'towards',\n",
              " 'condemn',\n",
              " 'nest',\n",
              " 'cheated',\n",
              " 'love',\n",
              " 'dive',\n",
              " 'sands',\n",
              " 'practice',\n",
              " 'passage',\n",
              " 'punished',\n",
              " 'impossible',\n",
              " 'opposite',\n",
              " 'afore',\n",
              " 'bit',\n",
              " 'ate',\n",
              " 'comes',\n",
              " 'something',\n",
              " 'all',\n",
              " 'above',\n",
              " 'second',\n",
              " 'unfolded',\n",
              " 'encouraging',\n",
              " 'will',\n",
              " 'kept',\n",
              " 'exactly',\n",
              " 'He',\n",
              " 'tried',\n",
              " 'breeze',\n",
              " 'speak',\n",
              " 'witness',\n",
              " 'shyly',\n",
              " 'side',\n",
              " 'neck',\n",
              " 'growl',\n",
              " 'happens',\n",
              " 'door',\n",
              " 'Elsie',\n",
              " 'violently',\n",
              " 'go',\n",
              " 'spectacles',\n",
              " 'before',\n",
              " 'ring',\n",
              " 'best',\n",
              " 'severity',\n",
              " 'hedges',\n",
              " 'Because',\n",
              " 'tidy',\n",
              " 'picked',\n",
              " 'faint',\n",
              " 'instead',\n",
              " 'Ann',\n",
              " 'ashamed',\n",
              " 'boon',\n",
              " 'comfortable',\n",
              " 'whether',\n",
              " 'climb',\n",
              " 'ceiling',\n",
              " 'accustomed',\n",
              " 'hide',\n",
              " 'elegant',\n",
              " 'become',\n",
              " 'advance',\n",
              " 'advise',\n",
              " 'complained',\n",
              " 'blacking',\n",
              " 'near',\n",
              " 'ground',\n",
              " 'daisies',\n",
              " 'important',\n",
              " 'tremble',\n",
              " 'clearly',\n",
              " 'entirely',\n",
              " 'addressing',\n",
              " 'girls',\n",
              " 'Her',\n",
              " 'staring',\n",
              " 'lullaby',\n",
              " 'tempered',\n",
              " 'contemptuous',\n",
              " 'not',\n",
              " 'lips',\n",
              " 'bowing',\n",
              " 'fireplace',\n",
              " 'apples',\n",
              " 'eye',\n",
              " 'rudeness',\n",
              " 'terms',\n",
              " 'always',\n",
              " 'begin',\n",
              " 'diligently',\n",
              " 'desperate',\n",
              " 'too',\n",
              " 'Mind',\n",
              " 'less',\n",
              " 'each',\n",
              " 'paper',\n",
              " 'caterpillar',\n",
              " 'trouble',\n",
              " 'because',\n",
              " 'grew',\n",
              " 'lasted',\n",
              " 'knowing',\n",
              " 'splashing',\n",
              " 'meal',\n",
              " 'lowing',\n",
              " 'happy',\n",
              " 'poor',\n",
              " 'alternately',\n",
              " 'frighten',\n",
              " 'nervous',\n",
              " 'teacup',\n",
              " 'Pinch',\n",
              " 'prosecute',\n",
              " 'shifting',\n",
              " 'late',\n",
              " 'stirring',\n",
              " 'sleepy',\n",
              " 'fumbled',\n",
              " 'Visit',\n",
              " 'tray',\n",
              " 'invitation',\n",
              " 'cushion',\n",
              " 'sister',\n",
              " 'state',\n",
              " 'oblong',\n",
              " 'placed',\n",
              " 'below',\n",
              " 'startled',\n",
              " 'hearth',\n",
              " 'age',\n",
              " 'roots',\n",
              " 'want',\n",
              " 'Still',\n",
              " 'took',\n",
              " 'Alice',\n",
              " 'remarking',\n",
              " 'unpleasant',\n",
              " 'maps',\n",
              " 'says',\n",
              " 'peering',\n",
              " 'spell',\n",
              " 'blades',\n",
              " 'crab',\n",
              " 'Distraction',\n",
              " 'Like',\n",
              " 'difficult',\n",
              " 'fortunately',\n",
              " 'creatures',\n",
              " 'eleventh',\n",
              " 'resting',\n",
              " 'waistcoat',\n",
              " 'fond',\n",
              " 'bite',\n",
              " 'standing',\n",
              " 'waited',\n",
              " 'feet',\n",
              " 'should',\n",
              " 'Stigand',\n",
              " 'my',\n",
              " 'presented',\n",
              " 'Father',\n",
              " 'vanishing',\n",
              " 'leant',\n",
              " 'Stop',\n",
              " 'pleasant',\n",
              " 'seem',\n",
              " 'stretching',\n",
              " 'lessen',\n",
              " 'further',\n",
              " 'sensation',\n",
              " 'laugh',\n",
              " 'Though',\n",
              " 'Pray',\n",
              " 'retire',\n",
              " 'Half',\n",
              " 'most',\n",
              " 'Morcar',\n",
              " 'rattle',\n",
              " 'brightened',\n",
              " 'tut',\n",
              " 'handsome',\n",
              " 'meet',\n",
              " 'panted',\n",
              " 'hid',\n",
              " 'commotion',\n",
              " 'terribly',\n",
              " 'howling',\n",
              " 'crying',\n",
              " 'cause',\n",
              " 'crumbs',\n",
              " 'jaws',\n",
              " 'Number',\n",
              " 'engaged',\n",
              " 'words',\n",
              " 'They',\n",
              " 'him',\n",
              " 'stay',\n",
              " 'cart',\n",
              " 'might',\n",
              " 'stiff',\n",
              " 'choice',\n",
              " 'overcome',\n",
              " 'trees',\n",
              " 'leave',\n",
              " 'dreadfully',\n",
              " 'being',\n",
              " 'slate',\n",
              " 'flurry',\n",
              " 'fur',\n",
              " 'turns',\n",
              " 'setting',\n",
              " 'tone',\n",
              " 'declare',\n",
              " 'thank',\n",
              " 'processions',\n",
              " 'your',\n",
              " 'Waiting',\n",
              " 'milk',\n",
              " 'stairs',\n",
              " 'crowded',\n",
              " 'played',\n",
              " 'follow',\n",
              " 'sharks',\n",
              " 'With',\n",
              " 'purple',\n",
              " 'morning',\n",
              " 'brass',\n",
              " 'hopeful',\n",
              " 'panting',\n",
              " 'nursing',\n",
              " 'manner',\n",
              " 'steady',\n",
              " 'constant',\n",
              " 'Silence',\n",
              " 'harm',\n",
              " 'currants',\n",
              " 'wept',\n",
              " 'losing',\n",
              " 'VII',\n",
              " 'Pepper',\n",
              " 'chimneys',\n",
              " 'doors',\n",
              " 'ridge',\n",
              " 'walked',\n",
              " 'dunce',\n",
              " 'stoop',\n",
              " 'chain',\n",
              " 'when',\n",
              " 'knee',\n",
              " 'swallowed',\n",
              " 'pictured',\n",
              " 'drowned',\n",
              " 'chanced',\n",
              " 'Long',\n",
              " 'nonsense',\n",
              " 'We',\n",
              " 'five',\n",
              " 'IV',\n",
              " 'conger',\n",
              " 'Go',\n",
              " 'disagree',\n",
              " 'entrance',\n",
              " 'spades',\n",
              " 'general',\n",
              " 'Advice',\n",
              " 'newspapers',\n",
              " 'busily',\n",
              " 'wonderful',\n",
              " 'pardon',\n",
              " 'settling',\n",
              " 'truthful',\n",
              " 'pause',\n",
              " 'paint',\n",
              " 'forehead',\n",
              " 'nasty',\n",
              " 'loveliest',\n",
              " 'White',\n",
              " 'severely',\n",
              " 'stalk',\n",
              " 'inquisitively',\n",
              " 'lose',\n",
              " 'whistling',\n",
              " 'Just',\n",
              " 'sea',\n",
              " 'patted',\n",
              " 'unlocking',\n",
              " 'game',\n",
              " 'remarkable',\n",
              " 'humbly',\n",
              " 'At',\n",
              " 'lower',\n",
              " 'sticks',\n",
              " 'saves',\n",
              " 'wanted',\n",
              " 'assembled',\n",
              " 'cleared',\n",
              " 'shrimp',\n",
              " 'mouse',\n",
              " 'Croquet',\n",
              " 'now',\n",
              " 'names',\n",
              " 'kiss',\n",
              " 'dig',\n",
              " 'Presently',\n",
              " 'croqueting',\n",
              " 'There',\n",
              " 'It',\n",
              " 'fat',\n",
              " 'spoke',\n",
              " 'party',\n",
              " 'pan',\n",
              " 'laughter',\n",
              " 'croquet',\n",
              " 'foot',\n",
              " 'knuckles',\n",
              " 'possibly',\n",
              " 'waist',\n",
              " 'hall',\n",
              " 'No',\n",
              " 'addressed',\n",
              " 'upon',\n",
              " 'thoughts',\n",
              " 'shrinking',\n",
              " 'eat',\n",
              " 'finding',\n",
              " 'than',\n",
              " 'singing',\n",
              " 'splendidly',\n",
              " 'smallest',\n",
              " 'fashion',\n",
              " 'cutting',\n",
              " 'bore',\n",
              " 'high',\n",
              " 'Edgar',\n",
              " 'removed',\n",
              " 'Miss',\n",
              " 'flinging',\n",
              " 'mustard',\n",
              " 'one',\n",
              " 'dish',\n",
              " 'life',\n",
              " 'accounting',\n",
              " 'family',\n",
              " 'daresay',\n",
              " 'riddles',\n",
              " 'QUEEN',\n",
              " 'sobbed',\n",
              " 'GUTENBERG',\n",
              " 'judging',\n",
              " 'introduce',\n",
              " 'Footman',\n",
              " 'nicely',\n",
              " 'cheerfully',\n",
              " 'opened',\n",
              " 'bawled',\n",
              " 'which',\n",
              " 'centre',\n",
              " 'treat',\n",
              " 'pigs',\n",
              " 'slipped',\n",
              " 'broke',\n",
              " 'kills',\n",
              " 'procession',\n",
              " 'pretty',\n",
              " 'know',\n",
              " 'doorway',\n",
              " 'sentence',\n",
              " 'chief',\n",
              " 'burn',\n",
              " 'were',\n",
              " 'seems',\n",
              " 'Now',\n",
              " 'Never',\n",
              " 'cross',\n",
              " 'Race',\n",
              " 'corner',\n",
              " 'desks',\n",
              " 'uneasy',\n",
              " 'master',\n",
              " 'please',\n",
              " 'dead',\n",
              " 'touch',\n",
              " 'catching',\n",
              " 'getting',\n",
              " 'bones',\n",
              " 'recovered',\n",
              " 'small',\n",
              " 'quarrelled',\n",
              " 'mallets',\n",
              " 'pointed',\n",
              " 'toast',\n",
              " 'fairly',\n",
              " 'memory',\n",
              " 'largest',\n",
              " 'earth',\n",
              " 'was',\n",
              " 'uncorked',\n",
              " 'cards',\n",
              " 'lazily',\n",
              " 'waters',\n",
              " 'nurse',\n",
              " 'impertinent',\n",
              " 'clock',\n",
              " 'pleasing',\n",
              " 'Will',\n",
              " 'so',\n",
              " 'peeped',\n",
              " 'IX',\n",
              " 'executions',\n",
              " 'remained',\n",
              " 'lit',\n",
              " 'rumbling',\n",
              " 'bottom',\n",
              " 'miserable',\n",
              " 'eyed',\n",
              " 'carrier',\n",
              " 'ALICE',\n",
              " 'softly',\n",
              " 'returned',\n",
              " 'toffee',\n",
              " 'bill',\n",
              " 'wasting',\n",
              " 'hoarsely',\n",
              " 'Therefore',\n",
              " 'changed',\n",
              " 'Mabel',\n",
              " 'wondered',\n",
              " 'Owl',\n",
              " 'perfectly',\n",
              " 'explanation',\n",
              " 'sounded',\n",
              " 'ignorant',\n",
              " 'mineral',\n",
              " 'dreaming',\n",
              " 'king',\n",
              " 'telescope',\n",
              " 'raving',\n",
              " 'belong',\n",
              " 'attempted',\n",
              " 'lessons',\n",
              " 'cry',\n",
              " 'respect',\n",
              " 'submitted',\n",
              " 'passing',\n",
              " 'trickling',\n",
              " 'else',\n",
              " 'venture',\n",
              " 'sitting',\n",
              " 'kid',\n",
              " 'rattling',\n",
              " 'interrupt',\n",
              " 'again',\n",
              " 'birds',\n",
              " 'pie',\n",
              " 'continued',\n",
              " 'laid',\n",
              " 'arrow',\n",
              " 'decided',\n",
              " 'furious',\n",
              " 'lost',\n",
              " 'why',\n",
              " 'impatiently',\n",
              " 'officers',\n",
              " 'water',\n",
              " 'unless',\n",
              " 'suddenly',\n",
              " 'pegs',\n",
              " 'sadly',\n",
              " 'Dinah',\n",
              " 'evening',\n",
              " 'bird',\n",
              " 'Eaglet',\n",
              " 'nevertheless',\n",
              " 'young',\n",
              " 'had',\n",
              " 'lives',\n",
              " 'energetic',\n",
              " 'appear',\n",
              " 'woke',\n",
              " 'planning',\n",
              " 'repeating',\n",
              " 'everybody',\n",
              " 'any',\n",
              " 'meant',\n",
              " 'bark',\n",
              " 'Adventures',\n",
              " 'timidly',\n",
              " 'worried',\n",
              " 'pretending',\n",
              " 'ugly',\n",
              " 'rearing',\n",
              " 'somersault',\n",
              " 'lying',\n",
              " 'Be',\n",
              " 'like',\n",
              " 'Would',\n",
              " 'suppose',\n",
              " 'fix',\n",
              " 'across',\n",
              " 'hers',\n",
              " 'gazing',\n",
              " 'soup',\n",
              " 'various',\n",
              " 'ready',\n",
              " 'stick',\n",
              " 'where',\n",
              " 'perhaps',\n",
              " 'squeaking',\n",
              " 'Dormouse',\n",
              " 'personal',\n",
              " 'moment',\n",
              " 'fellow',\n",
              " 'humble',\n",
              " 'truth',\n",
              " 'beast',\n",
              " 'sang',\n",
              " 'certain',\n",
              " 'calmly',\n",
              " 'brain',\n",
              " 'digging',\n",
              " 'worse',\n",
              " 'afraid',\n",
              " 'their',\n",
              " 'Jack',\n",
              " 'new',\n",
              " 'Soon',\n",
              " 'made',\n",
              " 'tired',\n",
              " 'shriek',\n",
              " 'father',\n",
              " 'recognised',\n",
              " 'means',\n",
              " 'letter',\n",
              " 'provoking',\n",
              " 'HEARTS',\n",
              " 'surprised',\n",
              " 'belongs',\n",
              " 'remember',\n",
              " 'fire',\n",
              " 'eels',\n",
              " 'passionate',\n",
              " 'alarm',\n",
              " 'CHORUS',\n",
              " 'forget',\n",
              " 'flew',\n",
              " 'spoken',\n",
              " 'snatch',\n",
              " 'absence',\n",
              " 'locks',\n",
              " 'inches',\n",
              " 'laughing',\n",
              " 'carefully',\n",
              " 'prove',\n",
              " 'pleased',\n",
              " 'thing',\n",
              " 'sizes',\n",
              " 'least',\n",
              " 'Dinn',\n",
              " 'two',\n",
              " 'deal',\n",
              " 'immediately',\n",
              " 'Fender',\n",
              " 'bear',\n",
              " 'hurt',\n",
              " 'dreadful',\n",
              " 'room',\n",
              " 'closer',\n",
              " 'tail',\n",
              " 'childhood',\n",
              " 'shorter',\n",
              " 'bread',\n",
              " 'sudden',\n",
              " 'mark',\n",
              " 'feared',\n",
              " 'name',\n",
              " 'sir',\n",
              " 'shouted',\n",
              " 'talk',\n",
              " 'Trims',\n",
              " 'Uglification',\n",
              " 'Hearts',\n",
              " 'to',\n",
              " 'contempt',\n",
              " 'arch',\n",
              " 'except',\n",
              " 'traps',\n",
              " 'shared',\n",
              " 'teaching',\n",
              " 'happen',\n",
              " 'sit',\n",
              " 'behind',\n",
              " 'shelves',\n",
              " 'times',\n",
              " 'But',\n",
              " 'stuff',\n",
              " 'fanning',\n",
              " 'deny',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GfkThRpNUKL"
      },
      "source": [
        "Define a function that takes a word and lists the $n$ most similar words in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_h-7n50kia"
      },
      "source": [
        "def spacy_closest(tokens, new_vec, n=10):\n",
        "  closest = sorted(tokens, key=lambda token: cosine(nlp(token).vector, new_vec), reverse=True)\n",
        "  return closest[:n]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTLEiz9UNjrO"
      },
      "source": [
        "Try to find words similar to some random words, e.g. $good$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1JL2VrF0ltD",
        "outputId": "8ed2b3d0-5712-488e-9f68-ad908cdd9fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spacy_closest(tokens, nlp('good').vector)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-958c4f6c629b>:4: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wonderful',\n",
              " 'pleasant',\n",
              " 'happy',\n",
              " 'best',\n",
              " 'poor',\n",
              " 'always',\n",
              " 'like',\n",
              " 'comfortable',\n",
              " 'unpleasant',\n",
              " 'fortunately']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZhjqSgNqNd"
      },
      "source": [
        "You can also get creative and search for combinations of words. For example, what is similar to $king - man + woman$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKI4SrhMN-EV",
        "outputId": "8a99cc90-7372-431f-c628-ae2eb8c092ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Your code here\n",
        "spacy_closest(tokens, nlp(\"king\").vector - nlp(\"man\").vector + nlp(\"woman\").vector, n=5)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-958c4f6c629b>:4: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['king', 'crown', 'consented', 'authority', 'ordering']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpD73pj8OGGt"
      },
      "source": [
        "## **Sentence vectors**\n",
        "\n",
        "We can also construct a vector representation for the whole sentence. For example, we can define it as an *average* of the   vectors representing the words in it.\n",
        "\n",
        "Let's take a random sentence *My favorite food is strawberry ice cream* and construct its vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5xr_3MkEPeM"
      },
      "source": [
        "sent = nlp('My favorite food is strawberry ice cream.')\n",
        "\n",
        "# Your code here\n",
        "whole = 'My favorite food is strawberry ice cream.'\n",
        "def sentence_vector(sentence):\n",
        "    calculator = lambda token: nlp(token).vector\n",
        "    return sum(map(calculator, sentence.split())) / len(sentence.split())\n",
        "\n",
        "sentv = sentence_vector(whole)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMf_OllyOvfX"
      },
      "source": [
        "Let's also extract sentences (as opposed to individual words) from our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jazUz0WvDsa3"
      },
      "source": [
        "sents = list(doc.sents)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzDdce7xO7QZ"
      },
      "source": [
        "Define a function that takes a random sentence and lists $n$ most similar sentences from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ7pQe8xD1x0"
      },
      "source": [
        "def spacy_closest_sent(sentences, input_vec, n=10):\n",
        "  print(type(sentences[0]))\n",
        "  closest = sorted(sentences, key=lambda sent: cosine(nlp(sent).vector, input_vec))\n",
        "  return sorted[:n]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6m06T18PDQ8"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkDCEWWwEzIc",
        "outputId": "4cf1dd07-3b1d-468a-ecac-0e3e3dd63b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "for s in spacy_closest_sent(sents, sentv, n=10):\n",
        "  print(s)\n",
        "  print('\\n---')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.span.Span'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'spacy.tokens.span.Span'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-ab3117c55ffd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_closest_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-97e7ece88bbd>\u001b[0m in \u001b[0;36mspacy_closest_sent\u001b[0;34m(sentences, input_vec, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_closest_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mclosest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-97e7ece88bbd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_closest_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mclosest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE1041\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     def _ensure_doc_with_context(\n",
            "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'spacy.tokens.span.Span'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VewB5XqkPLdx"
      },
      "source": [
        "## **References**\n",
        "\n",
        "This notebook is inspired by a [tutorial by Allison Parrish](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)."
      ]
    }
  ]
}